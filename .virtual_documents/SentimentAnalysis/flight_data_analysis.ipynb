# Filtering Warnings
import warnings
warnings.filterwarnings('ignore')

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns





import requests
from bs4 import BeautifulSoup
import csv

# Base URL with a placeholder for the page number
base_url = "https://www.airlinequality.com/airline-reviews/british-airways/page/{}/?sortby=post_date%3ADesc&pagesize=100"

# List to hold all reviews data
reviews_data = []

# Initialize a global serial number
serial_no = 1

# Function to fetch data from a specific page
def fetch_page(page_number):
    global serial_no  # Use the global serial number variable
    url = base_url.format(page_number)
    response = requests.get(url)

    # Check if the request was successful
    if response.status_code == 200:
        soup = BeautifulSoup(response.content, 'html.parser')

        # Find all review articles on the page
        review_articles = soup.find_all('article', class_='comp_media-review-rated')

        # Loop through each review article and extract the details
        for article in review_articles:
            review = {}
            review['Serial_No'] = serial_no
            
            # Scraping the "Trip Verified" status
            trip_verified = article.find('em').text.strip() if article.find('em') else None
            review['Trip_Verified'] = trip_verified

            # Scraping the feedback text and cleaning it
            feedback_text = article.find('div', itemprop="reviewBody").text.strip()
            feedback_text = feedback_text.replace(trip_verified, '').strip() if trip_verified else feedback_text
            feedback_text = feedback_text.lstrip('|').strip()
            review['Feedback'] = feedback_text
            
            review['Overall_Rating'] = article.find('div', class_='rating-10').find('span', itemprop='ratingValue').text.strip() if article.find('div', class_='rating-10') else None
            review['Date'] = article.find('meta', itemprop='datePublished')['content'] if article.find('meta', itemprop='datePublished') else None
            review['Type_of_Traveller'] = article.find('td', class_='review-rating-header type_of_traveller').find_next('td').text.strip() if article.find('td', class_='review-rating-header type_of_traveller') else None
            review['Seat_Type'] = article.find('td', class_='review-rating-header cabin_flown').find_next('td').text.strip() if article.find('td', class_='review-rating-header cabin_flown') else None
            review['Seat_Comfort'] = len(article.find('td', class_='review-rating-header seat_comfort').find_next('td').find_all('span', class_='star fill')) if article.find('td', class_='review-rating-header seat_comfort') else None
            review['Cabin_Staff_Service'] = len(article.find('td', class_='review-rating-header cabin_staff_service').find_next('td').find_all('span', class_='star fill')) if article.find('td', class_='review-rating-header cabin_staff_service') else None
            review['Ground_Service'] = len(article.find('td', class_='review-rating-header ground_service').find_next('td').find_all('span', class_='star fill')) if article.find('td', class_='review-rating-header ground_service') else None
            review['Wifi_Connectivity'] = len(article.find('td', class_='review-rating-header wifi_and_connectivity').find_next('td').find_all('span', class_='star fill')) if article.find('td', class_='review-rating-header wifi_and_connectivity') else None
            review['Food_and_Beverages'] = len(article.find('td', class_='review-rating-header food_and_beverages').find_next('td').find_all('span', class_='star fill')) if article.find('td', class_='review-rating-header food_and_beverages') else None
            review['Inflight_Entertainment'] = len(article.find('td', class_='review-rating-header inflight_entertainment').find_next('td').find_all('span', class_='star fill')) if article.find('td', class_='review-rating-header inflight_entertainment') else None
            review['Value_For_Money'] = len(article.find('td', class_='review-rating-header value_for_money').find_next('td').find_all('span', class_='star fill')) if article.find('td', class_='review-rating-header value_for_money') else None
            review['Recommended'] = article.find('td', class_='review-rating-header recommended').find_next('td').text.strip() if article.find('td', class_='review-rating-header recommended') else None

            # Add the review dictionary to the list
            reviews_data.append(review)
            serial_no += 1  # Increment the serial number for the next review

    else:
        print(f"Failed to retrieve page {page_number}")

# Loop to fetch data from pages 1 to 32
for page in range(1, 33):
    print(f"Fetching page {page}...")
    fetch_page(page)

# Save all reviews data to a CSV file
if reviews_data:
    keys = reviews_data[0].keys()
    with open('british_airways_reviews.csv', 'w', newline='', encoding='utf-8') as output_file:
        dict_writer = csv.DictWriter(output_file, fieldnames=keys)
        dict_writer.writeheader()
        dict_writer.writerows(reviews_data)
    print(f"Saved {len(reviews_data)} reviews to 'british_airways_reviews.csv'.")
else:
    print("No data to save.")


print(reviews_data[0])


df = pd.read_csv("british_airways_reviews.csv")
df.head()


df.shape


df.isnull().sum()


df["Type_of_Traveller"].unique()


df["Seat_Comfort"].unique()


data = df[["Serial_No", "Trip_Verified","Feedback"]]
data.head()


data.Trip_Verified.unique()


data.loc[:, 'Trip_Verified'] = data['Trip_Verified'].fillna('Not Verified')


data.Trip_Verified.value_counts()


data.isnull().sum()





import re
import nltk
from nltk.corpus import stopwords
import emoji

# Download stopwords if you haven't already
nltk.download('stopwords')

# Define a function to clean the text
def clean(text):
    # Remove emojis from the text
    text = emoji.replace_emoji(text, replace='')  # Remove all emojis

    # Removes all special characters and numericals leaving the alphabets
    text = re.sub('[^A-Za-z]+', ' ', str(text))
    
    # Convert to lowercase
    text = text.lower()

    # Tokenize the text into words
    words = text.split()

    # Remove stop words
    stop_words = set(stopwords.words('english'))
    words = [word for word in words if word not in stop_words]

    # Join the cleaned words back into a single string
    cleaned_text = ' '.join(words)

    return cleaned_text

# Create a new column 'Cleaned Reviews' by applying the clean function
data['Cleaned Reviews'] = data['Feedback'].apply(clean)


pd.set_option('display.max_colwidth', None)
data.head()





import spacy

# Load SpaCy model
nlp = spacy.load('en_core_web_sm')

def lemmatize_with_spacy(text):
    doc = nlp(text)
    lemmatized_text = ' '.join([token.lemma_ for token in doc if not token.is_stop])
    return lemmatized_text

# Apply the function to the 'Cleaned Reviews' column
data['Lemma'] = data['Cleaned Reviews'].apply(lemmatize_with_spacy)

print(data.head())


data.sample(5)


data.columns


# Separate verified and unverified reviews
verified_reviews = data[data['Trip_Verified'] == 'Trip Verified']
unverified_reviews = data[data['Trip_Verified'] == 'Not Verified']


nltk.download('vader_lexicon')





import pandas as pd
from textblob import TextBlob
from nltk.sentiment.vader import SentimentIntensityAnalyzer
import matplotlib.pyplot as plt

# Initialize VADER
sid = SentimentIntensityAnalyzer()

# Function to analyze sentiment using VADER and convert to labels
def analyze_sentiment_vader(text):
    score = sid.polarity_scores(text)['compound']
    if score >= 0.05:
        return 'Positive'
    elif score <= -0.05:
        return 'Negative'
    else:
        return 'Neutral'

# Function to analyze sentiment using TextBlob and convert to labels
def analyze_sentiment_textblob(text):
    score = TextBlob(text).sentiment.polarity
    if score > 0:
        return 'Positive'
    elif score < 0:
        return 'Negative'
    else:
        return 'Neutral'

# Filter the data into Trip Verified and Not Verified
verified_reviews = data[data['Trip_Verified'] == 'Trip Verified']
unverified_reviews = data[data['Trip_Verified'] == 'Not Verified']

# Apply sentiment analysis separately for verified and unverified reviews
verified_reviews['VADER_Sentiment'] = verified_reviews['Lemma'].apply(analyze_sentiment_vader)
verified_reviews['TextBlob_Sentiment'] = verified_reviews['Lemma'].apply(analyze_sentiment_textblob)

unverified_reviews['VADER_Sentiment'] = unverified_reviews['Lemma'].apply(analyze_sentiment_vader)
unverified_reviews['TextBlob_Sentiment'] = unverified_reviews['Lemma'].apply(analyze_sentiment_textblob)



verified_reviews.sample(5)


unverified_reviews.sample(5)


verified_reviews['VADER_Sentiment'].value_counts()


verified_reviews['TextBlob_Sentiment'].value_counts()


unverified_reviews['VADER_Sentiment'].value_counts()


unverified_reviews['TextBlob_Sentiment'].value_counts()


# Visualize the sentiment distribution for VADER with percentage labels

# For Trip Verified Reviews
verified_vader_sentiment_counts = verified_reviews['VADER_Sentiment'].value_counts(normalize=True) * 100
ax = verified_vader_sentiment_counts.plot(kind='bar', color=['#4CAF50', '#FFC107', '#F44336'])
plt.title('VADER Sentiment Distribution (Trip Verified)')
plt.xlabel('Sentiment')
plt.ylabel('Percentage')
for p in ax.patches:
    ax.annotate(f'{p.get_height():.2f}%', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='baseline')
plt.show()

# For Not Verified Reviews
unverified_vader_sentiment_counts = unverified_reviews['VADER_Sentiment'].value_counts(normalize=True) * 100
ax = unverified_vader_sentiment_counts.plot(kind='bar', color=['#4CAF50', '#FFC107', '#F44336'])
plt.title('VADER Sentiment Distribution (Not Verified)')
plt.xlabel('Sentiment')
plt.ylabel('Percentage')
for p in ax.patches:
    ax.annotate(f'{p.get_height():.2f}%', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='baseline')
plt.show()

# Visualize the sentiment distribution for TextBlob with percentage labels

# For Trip Verified Reviews
verified_textblob_sentiment_counts = verified_reviews['TextBlob_Sentiment'].value_counts(normalize=True) * 100
ax = verified_textblob_sentiment_counts.plot(kind='bar', color=['#4CAF50', '#FFC107', '#F44336'])
plt.title('TextBlob Sentiment Distribution (Trip Verified)')
plt.xlabel('Sentiment')
plt.ylabel('Percentage')
for p in ax.patches:
    ax.annotate(f'{p.get_height():.2f}%', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='baseline')
plt.show()

# For Not Verified Reviews
unverified_textblob_sentiment_counts = unverified_reviews['TextBlob_Sentiment'].value_counts(normalize=True) * 100
ax = unverified_textblob_sentiment_counts.plot(kind='bar', color=['#4CAF50', '#FFC107', '#F44336'])
plt.title('TextBlob Sentiment Distribution (Not Verified)')
plt.xlabel('Sentiment')
plt.ylabel('Percentage')
for p in ax.patches:
    ax.annotate(f'{p.get_height():.2f}%', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='baseline')
plt.show()






import gensim
from gensim import corpora

# Create a dictionary and corpus for LDA
def prepare_lda_data(reviews):
    text_data = [review.split() for review in reviews['Lemma']]
    dictionary = corpora.Dictionary(text_data)
    corpus = [dictionary.doc2bow(text) for text in text_data]
    return dictionary, corpus

# For verified reviews
verified_dict, verified_corpus = prepare_lda_data(verified_reviews)
unverified_dict, unverified_corpus = prepare_lda_data(unverified_reviews)

# Train LDA models
verified_lda_model = gensim.models.ldamodel.LdaModel(corpus=verified_corpus, num_topics=5, id2word=verified_dict, passes=15)
unverified_lda_model = gensim.models.ldamodel.LdaModel(corpus=unverified_corpus, num_topics=5, id2word=unverified_dict, passes=15)

# Print the topics
print('Verified LDA Model')
print('---------------------')
print(verified_lda_model.print_topics())
print('Unverified LDA Model')
print('---------------------')
print(unverified_lda_model.print_topics())


import matplotlib.pyplot as plt
import numpy as np

def plot_topic_distribution_with_labels(lda_model, corpus, title):
    # Get the topic distribution
    topic_distribution = [lda_model.get_document_topics(doc) for doc in corpus]
    topic_counts = [max(topics, key=lambda x: x[1])[0] for topics in topic_distribution]
    
    # Get topic names
    topic_labels = []
    for i in range(lda_model.num_topics):
        words = lda_model.show_topic(i, topn=3)  # Get the top 3 words for each topic
        topic_label = ", ".join([word for word, _ in words])
        topic_labels.append(topic_label)
    
    # Calculate counts for each topic
    unique, counts = np.unique(topic_counts, return_counts=True)
    
    # Colors for the bars
    colors = plt.cm.viridis(np.linspace(0, 1, lda_model.num_topics))
    
    # Plotting
    plt.figure(figsize=(12, 8))
    bars = plt.bar(unique, counts, color=colors, edgecolor='black')
    plt.xlabel('Topic Number')
    plt.ylabel('Number of Reviews')
    plt.title(title)
    
    # Add labels to the bars
    plt.xticks(range(lda_model.num_topics), topic_labels, rotation=30, ha="center")
    
    plt.show()

# For verified reviews
plot_topic_distribution_with_labels(verified_lda_model, verified_corpus, 'Topic Distribution for Verified Reviews')

# For unverified reviews
plot_topic_distribution_with_labels(unverified_lda_model, unverified_corpus, 'Topic Distribution for Unverified Reviews')






from wordcloud import WordCloud
import matplotlib.pyplot as plt

# Generate word clouds
def generate_wordcloud(text_data, title):
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(text_data))
    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    plt.title(title)
    plt.show()

# For verified reviews
generate_wordcloud(verified_reviews['Lemma'], 'Verified Reviews')

# For unverified reviews
generate_wordcloud(unverified_reviews['Lemma'], 'Unverified Reviews')






model_df = df


model_df.shape


model_df.sample(5).T


target_counts = model_df['Recommended'].value_counts()
target_counts


# Create a pie chart
plt.figure(figsize=(6, 6))
plt.pie(target_counts, labels=target_counts.index, autopct='%1.1f%%', startangle=140)
plt.title('Distribution of TARGET Variable')
plt.show()





model_df.duplicated().any()


#null checking

(model_df.isnull().sum()/len(model_df)*100).sort_values(ascending = False)


model_df['Trip_Verified'].unique()


'''
Since null observations can be said to be not verified, we input 'Not Verified' for null values.
'''

model_df.loc[:, 'Trip_Verified'] = model_df['Trip_Verified'].fillna('Not Verified')


'''
we drop 'Wifi_Connectivity' column for having too many null values. Also, we drop 
columns 'Serial_No', Feedback' and 'Date' due to less relevance for modelling.
'''

model_df.drop(columns=["Wifi_Connectivity","Serial_No", "Feedback","Date"],inplace=True,axis=1)
model_df.shape


model_df.dtypes


model_df.isnull().sum().sort_values(ascending = False)


'''
For numeric columns, we replace null values with 'mode'
'''
numeric_null_columns = ['Inflight_Entertainment', 'Food_and_Beverages', 'Ground_Service', 'Cabin_Staff_Service','Seat_Comfort']

model_df.Inflight_Entertainment.fillna(model_df.Inflight_Entertainment.mode()[0],inplace = True)
model_df.Food_and_Beverages.fillna(model_df.Food_and_Beverages.mode()[0],inplace = True)
model_df.Ground_Service.fillna(model_df.Ground_Service.mode()[0],inplace = True)
model_df.Cabin_Staff_Service.fillna(model_df.Cabin_Staff_Service.mode()[0],inplace = True)
model_df.Seat_Comfort.fillna(model_df.Seat_Comfort.mode()[0],inplace = True)


model_df['Type_of_Traveller'].value_counts(dropna=False)


model_df['Seat_Type'].value_counts(dropna=False)


'''
We input 'Couple Leisure' for null values since it is the highest occuring 'Type_of_Traveller'.
We input 'Economy Class' for null values since it is the highest occuring 'Seat_Type
'''

model_df.Type_of_Traveller.fillna('Couple Leisure',inplace = True)
model_df.Seat_Type.fillna('Economy Class',inplace = True)


model_df.isnull().sum()





import seaborn as sns

fig, ax = plt.subplots(figsize=(12,12))
mask = np.triu(np.ones_like(model_df.corr(numeric_only=True), dtype=bool))
sns.heatmap(model_df.corr(numeric_only=True), linewidths=0.5, mask=mask, square=True, ax=ax, annot=True);


#Casting target variable as int datatype

model_df['Recommended'] = (model_df['Recommended'] == 'yes').astype(int)


model_df['Recommended']


#correlated columns with target

correlation = model_df.corr(numeric_only=True)
correlation['Recommended'].sort_values(ascending=False)


'''
Since 'Overall_Rating' feature is highly correlated with other features which
might lead to overfitting, we consider dropping it.
'''

model_df.drop(['Overall_Rating'], axis=1)





y = model_df['Recommended']
X = model_df.drop(['Recommended'], axis=1)


cat_cols = X.select_dtypes(include='object')
cat_cols.columns


model_df_encoded = pd.get_dummies(X,columns=cat_cols.columns,dtype= 'int')
model_df_encoded.sample(5)


from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split


'''
scale, fit and transform data
'''

scaler = StandardScaler()

scaled_df = scaler.fit_transform(model_df_encoded)


scaled_df = pd.DataFrame(scaled_df, columns = model_df_encoded.columns)


scaled_df.head()





from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.metrics import ConfusionMatrixDisplay
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import precision_score, recall_score, f1_score
from sklearn.metrics import roc_auc_score
from sklearn.calibration import CalibratedClassifierCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import SGDClassifier


X_features = scaled_df.to_numpy()
y_label = y.values.reshape(-1,)

X_train, X_temp, y_train, y_temp = train_test_split(X_features, y_label, stratify = y_label, test_size=0.3, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, stratify = y_temp, test_size=0.5, random_state=42)
print('Shape of X_train:',X_train.shape)
print('Shape of X_val:',X_val.shape)
print('Shape of X_test:',X_test.shape)


model_results = [[],[]]





def plot_classification_metrics(test_y, predicted_y):
    # Confusion matrix
    C = confusion_matrix(test_y, predicted_y)

    # Precision matrix
    A = (C/C.sum(axis=0))
    
    # Recall matrix
    B = (((C.T)/(C.sum(axis=1))).T)
    
    plt.figure(figsize=(20,4))
    
    labels = ['Not Recommended', 'Recommended']
   
    cmap=sns.light_palette("green")
    plt.subplot(1,3,1)
    sns.heatmap(C, annot=True, cmap=cmap,fmt="d", xticklabels = labels, yticklabels=labels)
    plt.xlabel('Predicted Class')
    plt.ylabel('Original Class')
    plt.title('Confusion matrix')
    
    plt.subplot(1,3,2)
    sns.heatmap(A, annot=True, cmap=cmap, xticklabels = labels, yticklabels=labels)
    plt.xlabel('Predicted Class')
    plt.ylabel('Original Class')
    plt.title('Precision matrix')
    
    plt.subplot(1,3,3)
    sns.heatmap(B, annot=True, cmap=cmap, xticklabels = labels, yticklabels=labels)
    plt.xlabel('Predicted Class')
    plt.ylabel('Original Class')
    plt.title('Recall matrix')
    
    plt.show()





alpha = np.logspace(-4,4,9)
cv_auc_score = []
for i in alpha:
    clf = SGDClassifier(alpha=i, penalty='l1',class_weight = 'balanced', loss='log_loss', random_state=28)
    clf.fit(X_train, y_train)
    sig_clf = CalibratedClassifierCV(clf, method='sigmoid')
    sig_clf.fit(X_train, y_train)
    y_pred_prob = sig_clf.predict_proba(X_val)[:,1]
    cv_auc_score.append(roc_auc_score(y_val,y_pred_prob))
    print('For alpha {0}, cross validation AUC score {1}'.format(i,roc_auc_score(y_val,y_pred_prob)))
print('The Optimal C value is:', alpha[np.argmax(cv_auc_score)])


best_alpha = alpha[np.argmax(cv_auc_score)]
logreg = SGDClassifier(alpha = best_alpha, class_weight = 'balanced', penalty = 'l1', loss='log_loss', random_state = 28)
logreg.fit(X_train, y_train)
logreg_sig_clf = CalibratedClassifierCV(logreg, method='sigmoid')
logreg_sig_clf.fit(X_train, y_train)
y_pred_prob = logreg_sig_clf.predict_proba(X_train)[:,1]
print('For best alpha {0}, The Train AUC score is {1}'.format(best_alpha, roc_auc_score(y_train,y_pred_prob)))
model_results[0].append(roc_auc_score(y_train,y_pred_prob))
y_pred_prob = logreg_sig_clf.predict_proba(X_val)[:,1]
print('For best alpha {0}, The Cross validated AUC score is {1}'.format(best_alpha, roc_auc_score(y_val,y_pred_prob)))
model_results[0].append(roc_auc_score(y_val,y_pred_prob))
y_pred_prob = logreg_sig_clf.predict_proba(X_test)[:,1]
print('For best alpha {0}, The Test AUC score is {1}'.format(best_alpha, roc_auc_score(y_test,y_pred_prob)))
model_results[0].append(roc_auc_score(y_test,y_pred_prob))
y_pred = logreg.predict(X_test)
print('The test AUC score is :', roc_auc_score(y_test,y_pred_prob))
print('The test accuracy score is :', accuracy_score(y_test, y_pred))
model_results[0].append(accuracy_score(y_test,y_pred))
print('The percentage of misclassified points {:05.2f}% :'.format((1-accuracy_score(y_test, y_pred))*100))


plot_classification_metrics(y_test, y_pred)


from sklearn.metrics import roc_curve
fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)
auc = roc_auc_score(y_test,y_pred_prob)
plt.figure(figsize=(8,6))
plt.plot(fpr, tpr, marker='.')
plt.plot([0, 1], [0, 1], linestyle='--')
plt.title('ROC curve', fontsize = 20)
plt.xlabel('FPR', fontsize=15)
plt.ylabel('TPR', fontsize=15)
plt.grid()
plt.legend(["AUC=get_ipython().run_line_magic(".3f"%auc])", "")
plt.show()





alpha = [200,500,1000,2000]
max_depth = [7, 10]
cv_auc_score = []
for i in alpha:
    for j in max_depth:
        clf = RandomForestClassifier(n_estimators=i, criterion='gini', max_depth=j,class_weight='balanced',
                                     random_state=42, n_jobs=-1)
        clf.fit(X_train, y_train)
        sig_clf = CalibratedClassifierCV(clf, method="sigmoid")
        sig_clf.fit(X_train, y_train)
        y_pred_prob = sig_clf.predict_proba(X_val)[:,1]
        cv_auc_score.append(roc_auc_score(y_val,y_pred_prob))
        print('For n_estimators {0}, max_depth {1} cross validation AUC score {2}'.
              format(i,j,roc_auc_score(y_val,y_pred_prob)))


best_alpha = np.argmax(cv_auc_score)
print('The optimal values are: n_estimators {0}, max_depth {1} '.format(alpha[int(best_alpha/2)],
                                                                        max_depth[int(best_alpha%2)]))
rf = RandomForestClassifier(n_estimators=alpha[int(best_alpha/2)], criterion='gini', max_depth=max_depth[int(best_alpha%2)],
                            class_weight='balanced', random_state=42, n_jobs=-1)
rf.fit(X_train, y_train)
rf_sig_clf = CalibratedClassifierCV(rf, method="sigmoid")
rf_sig_clf.fit(X_train, y_train)
y_pred_prob = rf_sig_clf.predict_proba(X_train)[:,1]
print('For best n_estimators {0} best max_depth {1}, The Train AUC score is {2}'.format(alpha[int(best_alpha/2)],
                                                    max_depth[int(best_alpha%2)],roc_auc_score(y_train,y_pred_prob)))
model_results[1].append(roc_auc_score(y_train,y_pred_prob))
y_pred_prob = rf_sig_clf.predict_proba(X_val)[:,1]
print('For best n_estimators {0} best max_depth {1}, The Validation AUC score is {2}'.format(alpha[int(best_alpha/2)],
                                                            max_depth[int(best_alpha%2)],roc_auc_score(y_val,y_pred_prob)))
model_results[1].append(roc_auc_score(y_val,y_pred_prob))
y_pred_prob = rf_sig_clf.predict_proba(X_test)[:,1]
print('For best n_estimators {0} best max_depth {1}, The Test AUC score is {2}'.format(alpha[int(best_alpha/2)],
                                                        max_depth[int(best_alpha%2)],roc_auc_score(y_test,y_pred_prob)))
model_results[1].append(roc_auc_score(y_test,y_pred_prob))
y_pred = rf_sig_clf.predict(X_test)
print('The test AUC score is :', roc_auc_score(y_test,y_pred_prob))
print('The accuracy score is :', accuracy_score(y_test, y_pred))
model_results[1].append(accuracy_score(y_test,y_pred))
print('The percentage of misclassified points {:05.2f}% :'.format((1-accuracy_score(y_test, y_pred))*100))





plot_classification_metrics(y_test, y_pred)


from sklearn.metrics import roc_curve
fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)
auc = roc_auc_score(y_test,y_pred_prob)
plt.figure(figsize=(8,6))
plt.plot(fpr, tpr, marker='.')
plt.plot([0, 1], [0, 1], linestyle='--')
plt.title('ROC curve', fontsize = 20)
plt.xlabel('FPR', fontsize=15)
plt.ylabel('TPR', fontsize=15)
plt.grid()
plt.legend(["AUC=get_ipython().run_line_magic(".3f"%auc])", "")
plt.show()





eval_scores = pd.DataFrame(model_results, columns=['Train AUC','Cross Val AUC','Test AUC','Accuracy'], index=['Logistic Regression','Random Forest'])
eval_scores
