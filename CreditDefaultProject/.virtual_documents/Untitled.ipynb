# Filtering Warnings
import warnings
warnings.filterwarnings('ignore')





#Import Libraries
import numpy as np
import pandas as pd
import seaborn as sns
from matplotlib import pyplot as plt
from plotly.subplots import make_subplots
import plotly.graph_objects as go
pd.set_option('display.max_columns', 300) #Setting column display limit
plt.style.use('ggplot') #Applying style to graphs





app_df = pd.read_csv('application_data.csv')





#dimension of dataset 

app_df.shape 


#preview first and last 5 rows

app_df


app_df.info(verbose=True)


#Statistical description

app_df.describe()





target_counts = app_df['TARGET'].value_counts()
target_counts


# Create a pie chart
plt.figure(figsize=(6, 6))
plt.pie(target_counts, labels=target_counts.index, autopct='%1.1f%%', startangle=140)
plt.title('Distribution of TARGET Variable')
plt.show()





Target0 = app_df.loc[app_df["TARGET"]==0] 
Target1 = app_df.loc[app_df["TARGET"]==1]

round(len(Target0)/len(Target1),2)








#null checking

(app_df.isnull().sum()/len(app_df)*100).sort_values(ascending = False).head(50)





null_col = app_df.isnull().sum().sort_values(ascending = False)
null_col = null_col[null_col.values >(0.40*len(app_df))]


#Plotting Bar Graph for null values greater than 40%

plt.figure(figsize=(20,4))
null_col.plot(kind='bar', color="#4CB391")                           
plt.title('List of Columns & null counts where null values are more than 40%') 

plt.xlabel("Null Columns",fontdict={"fontsize":12,"fontweight":5}) 
plt.ylabel("Count of null values",fontdict={"fontsize":12,"fontweight":5})
plt.show()


len(null_col)


cols_to_drop = list(null_col.index.values) #Making list of column names having null values greater than 40%

app_df.drop(labels = cols_to_drop,axis=1,inplace = True) #Dropping those columns 


app_df.shape


#Checking for left-out columns with null

null_left = (app_df.isnull().sum()/len(app_df)*100).sort_values(ascending = False).head(20)
null_left


#OCCUPATION_TYPE COLUMN

app_df['OCCUPATION_TYPE'].isnull().sum()


app_df.OCCUPATION_TYPE.value_counts()


app_df.NAME_INCOME_TYPE.value_counts()


app_df[['NAME_INCOME_TYPE','OCCUPATION_TYPE']].head(20)


app_df[['NAME_INCOME_TYPE','OCCUPATION_TYPE']].tail(20)





app_df['OCCUPATION_TYPE'].fillna('Pensioner',inplace=True)


app_df[['NAME_INCOME_TYPE','OCCUPATION_TYPE']].head(20)


app_df['OCCUPATION_TYPE'].isnull().sum()


#EXT_SOURCE_3 AND EXT_SOURCE_2
#This are continous variables, we check for skewness

app_df['EXT_SOURCE_3'].skew()


app_df['EXT_SOURCE_2'].skew()





app_df.EXT_SOURCE_3.fillna(app_df.EXT_SOURCE_3.median() , inplace = True)

app_df.EXT_SOURCE_2.fillna(app_df.EXT_SOURCE_2.median() , inplace = True)


#AMT_REQ_CREDIT_BUREAU_HOUR AMT_REQ_CREDIT_BUREAU_DAY AMT_REQ_CREDIT_BUREAU_WEEK 
#AMT_REQ_CREDIT_BUREAU_MON AMT_REQ_CREDIT_BUREAU_QRT AMT_REQ_CREDIT_BUREAU_YEAR

#Since these are numeric columns and percentage of null values are below 15% for all columns, filling NaN values with Mode is advisable.

app_df.AMT_REQ_CREDIT_BUREAU_YEAR.fillna(app_df.AMT_REQ_CREDIT_BUREAU_YEAR.mode()[0],inplace = True)

app_df.AMT_REQ_CREDIT_BUREAU_MON.fillna(app_df.AMT_REQ_CREDIT_BUREAU_MON.mode()[0],inplace = True) 

app_df.AMT_REQ_CREDIT_BUREAU_WEEK.fillna(app_df.AMT_REQ_CREDIT_BUREAU_WEEK.mode()[0],inplace = True)

app_df.AMT_REQ_CREDIT_BUREAU_DAY.fillna(app_df.AMT_REQ_CREDIT_BUREAU_DAY.mode()[0],inplace = True) 

app_df.AMT_REQ_CREDIT_BUREAU_HOUR.fillna(app_df.AMT_REQ_CREDIT_BUREAU_HOUR.mode()[0],inplace = True) 

app_df.AMT_REQ_CREDIT_BUREAU_QRT.fillna(app_df.AMT_REQ_CREDIT_BUREAU_QRT.mode()[0],inplace = True)


#NAME_TYPE_SUITE 

#This is a categorical column, we will replace with Mode

app_df.NAME_TYPE_SUITE.fillna(app_df.NAME_TYPE_SUITE.mode()[0],inplace = True)


#OBS_30_CNT_SOCIAL_CIRCLE DEF_30_CNT_SOCIAL_CIRCLE 
#OBS_60_CNT_SOCIAL_CIRCLE DEF_60_CNT_SOCIAL_CIRCLE

#These are count columns based on observed and defaulted DPD, It's better to replace NaN with Zero by assuming missing values are as a result of no observable or defaulted connections within the specified days.

OBS_DEF_columns = ['OBS_30_CNT_SOCIAL_CIRCLE', 'DEF_30_CNT_SOCIAL_CIRCLE', 'OBS_60_CNT_SOCIAL_CIRCLE', 'DEF_60_CNT_SOCIAL_CIRCLE']

for column in OBS_DEF_columns:
    app_df[column].fillna(0, inplace=True)


#AMT_ANNUITY AMT_GOODS_PRICE
#These are continuous variables, we check for skewness

app_df['AMT_ANNUITY'].skew()


app_df['AMT_GOODS_PRICE'].skew()





app_df['AMT_ANNUITY'].fillna(app_df['AMT_ANNUITY'].median(),inplace=True)
app_df['AMT_GOODS_PRICE'].fillna(app_df['AMT_GOODS_PRICE'].median(),inplace=True)


#CNT_FAM_MEMBERS
#This is a numeric count column, we replace with Mode

app_df.CNT_FAM_MEMBERS.fillna(app_df.CNT_FAM_MEMBERS.mode()[0] , inplace = True)


#DAYS_LAST_PHONE_CHANGE

app_df['DAYS_LAST_PHONE_CHANGE'].head(5)


'''
This is a numeric column, 
1. values will be standardized by taking absolute values
2. replace few missing values with Mode
'''
app_df['DAYS_LAST_PHONE_CHANGE']=app_df['DAYS_LAST_PHONE_CHANGE'].abs()

app_df['DAYS_LAST_PHONE_CHANGE'].fillna(app_df['DAYS_LAST_PHONE_CHANGE'].mode,inplace=True)


final_null_check = (app_df.isnull().sum()/len(app_df)*100).sort_values(ascending = False)
final_null_check








#Due to observation on "DAYS_LAST_PHONE_CHANGE" column, there is a need to inspect similar columns with days

app_df_days = [i for i in app_df if i.startswith('DAYS')]

app_df_days.remove("DAYS_LAST_PHONE_CHANGE")


app_df[app_df_days]





app_df[app_df_days] = abs(app_df[app_df_days])

app_df[app_df_days].head()





#GENDER

app_df['CODE_GENDER'].value_counts()


#We replace the XNA values with 'F' which is the highest occuring gender

app_df.loc[app_df.CODE_GENDER == 'XNA','CODE_GENDER'] = 'F' 
app_df['CODE_GENDER'].value_counts()


#ORGANIZATION_TYPE

app_df['ORGANIZATION_TYPE'].value_counts().head()


app_df[['ORGANIZATION_TYPE','NAME_INCOME_TYPE']].head(20)





app_df['ORGANIZATION_TYPE'] = app_df['ORGANIZATION_TYPE'].replace('XNA', 'Pensioner')
app_df['ORGANIZATION_TYPE'].value_counts().head()





app_df['DAYS_BIRTH']= (app_df['DAYS_BIRTH']/365).astype(int)
app_df['DAYS_BIRTH'].unique()


app_df['AGE_GROUP']=pd.cut(app_df['DAYS_BIRTH'],                 
                         bins=[19,25,40,60,100], labels=['Very_Young','Youth', 'Middle_Age', 'Elder'])


app_df[['DAYS_BIRTH','AGE_GROUP']]





#using a heatmap to show correlation

fig, ax = plt.subplots(figsize=(12,12))
mask = np.triu(np.ones_like(app_df.corr(numeric_only=True), dtype=bool))
sns.heatmap(app_df.corr(numeric_only=True), linewidths=0.5, mask=mask, square=True, ax=ax, annot=True);


#CORRELATED COLUMNS WITH TARGET

correlation=app_df.corr(numeric_only=True)
correlation.TARGET.sort_values(ascending=True).head()






app_df.drop(columns=["EXT_SOURCE_2","EXT_SOURCE_3"],inplace=True,axis=1)
app_df.shape


#OUTLIERS ANALYSIS
'''
numerical_col = app_df.select_dtypes(include='number').columns
len(numerical_col)
fig , axes = plt.subplots(nrows=7, ncols=5, constrained_layout=True)              
fig.subplots_adjust(left= 0, bottom=0, right=3, top=12, wspace=0.09, hspace=0.3)


for ax, column in zip(axes.flatten(),numerical_col):    

    sns.boxplot(app_df[column],ax=ax)
'''
# Identify the outliers
Q1 = app_df['TARGET'].quantile(0.25)
Q3 = app_df['TARGET'].quantile(0.75)
IQR = Q3 - Q1
outliers = app_df[(app_df['TARGET'] < (Q1 - 1.5 * IQR)) | (app_df['TARGET'] > (Q3 + 1.5 * IQR))]
outliers['TARGET'].unique()



