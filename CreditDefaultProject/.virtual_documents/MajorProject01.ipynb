


# Filtering Warnings
import warnings
warnings.filterwarnings('ignore')

#Import Libraries
import numpy as np
import pandas as pd
import seaborn as sns
from matplotlib import pyplot as plt
from plotly.subplots import make_subplots
import plotly.graph_objects as go
pd.set_option('display.max_columns', 300) #Setting column display limit
plt.style.use('ggplot') #Applying style to graphs





app_df = pd.read_csv('application_data.csv')





#dimension of dataset 

app_df.shape 


#preview first and last 5 rows

app_df


app_df.info(verbose=True)


#Statistical description

app_df.describe()





target_counts = app_df['TARGET'].value_counts()
target_counts


# Create a pie chart
plt.figure(figsize=(6, 6))
plt.pie(target_counts, labels=target_counts.index, autopct='%1.1f%%', startangle=140)
plt.title('Distribution of TARGET Variable')
plt.show()





Target0 = app_df.loc[app_df["TARGET"]==0] 
Target1 = app_df.loc[app_df["TARGET"]==1]

round(len(Target0)/len(Target1),2)








#null checking

(app_df.isnull().sum()/len(app_df)*100).sort_values(ascending = False).head(50)





null_col = app_df.isnull().sum().sort_values(ascending = False)
null_col = null_col[null_col.values >(0.40*len(app_df))]


#Plotting Bar Graph for null values greater than 40%

plt.figure(figsize=(20,4))
null_col.plot(kind='bar', color="#4CB391")                           
plt.title('List of Columns & null counts where null values are more than 40%') 

plt.xlabel("Null Columns",fontdict={"fontsize":12,"fontweight":5}) 
plt.ylabel("Count of null values",fontdict={"fontsize":12,"fontweight":5})
plt.show()


len(null_col)


cols_to_drop = list(null_col.index.values) #Making list of column names having null values greater than 40%

app_df.drop(labels = cols_to_drop,axis=1,inplace = True) #Dropping those columns 


app_df.shape


#Checking for left-out columns with null

null_left = (app_df.isnull().sum()/len(app_df)*100).sort_values(ascending = False).head(20)
null_left


#OCCUPATION_TYPE COLUMN

app_df['OCCUPATION_TYPE'].isnull().sum()


app_df.OCCUPATION_TYPE.value_counts()


app_df.NAME_INCOME_TYPE.value_counts()


app_df[['NAME_INCOME_TYPE','OCCUPATION_TYPE']].head(20)


app_df[['NAME_INCOME_TYPE','OCCUPATION_TYPE']].tail(20)





app_df['OCCUPATION_TYPE'].fillna('Pensioner',inplace=True)


app_df[['NAME_INCOME_TYPE','OCCUPATION_TYPE']].head(20)


app_df['OCCUPATION_TYPE'].isnull().sum()


#EXT_SOURCE_3 AND EXT_SOURCE_2
#This are continous variables, we check for skewness

app_df['EXT_SOURCE_3'].skew()


app_df['EXT_SOURCE_2'].skew()





app_df.EXT_SOURCE_3.fillna(app_df.EXT_SOURCE_3.median() , inplace = True)

app_df.EXT_SOURCE_2.fillna(app_df.EXT_SOURCE_2.median() , inplace = True)


#AMT_REQ_CREDIT_BUREAU_HOUR AMT_REQ_CREDIT_BUREAU_DAY AMT_REQ_CREDIT_BUREAU_WEEK 
#AMT_REQ_CREDIT_BUREAU_MON AMT_REQ_CREDIT_BUREAU_QRT AMT_REQ_CREDIT_BUREAU_YEAR

#Since these are numeric columns and percentage of null values are below 15% for all columns, filling NaN values with Mode is advisable.

app_df.AMT_REQ_CREDIT_BUREAU_YEAR.fillna(app_df.AMT_REQ_CREDIT_BUREAU_YEAR.mode()[0],inplace = True)

app_df.AMT_REQ_CREDIT_BUREAU_MON.fillna(app_df.AMT_REQ_CREDIT_BUREAU_MON.mode()[0],inplace = True) 

app_df.AMT_REQ_CREDIT_BUREAU_WEEK.fillna(app_df.AMT_REQ_CREDIT_BUREAU_WEEK.mode()[0],inplace = True)

app_df.AMT_REQ_CREDIT_BUREAU_DAY.fillna(app_df.AMT_REQ_CREDIT_BUREAU_DAY.mode()[0],inplace = True) 

app_df.AMT_REQ_CREDIT_BUREAU_HOUR.fillna(app_df.AMT_REQ_CREDIT_BUREAU_HOUR.mode()[0],inplace = True) 

app_df.AMT_REQ_CREDIT_BUREAU_QRT.fillna(app_df.AMT_REQ_CREDIT_BUREAU_QRT.mode()[0],inplace = True)


#NAME_TYPE_SUITE 

#This is a categorical column, we will replace with Mode

app_df.NAME_TYPE_SUITE.fillna(app_df.NAME_TYPE_SUITE.mode()[0],inplace = True)


#OBS_30_CNT_SOCIAL_CIRCLE DEF_30_CNT_SOCIAL_CIRCLE 
#OBS_60_CNT_SOCIAL_CIRCLE DEF_60_CNT_SOCIAL_CIRCLE

#These are count columns based on observed and defaulted DPD, It's better to replace NaN with Zero by assuming missing values are as a result of no observable or defaulted connections within the specified days.

OBS_DEF_columns = ['OBS_30_CNT_SOCIAL_CIRCLE', 'DEF_30_CNT_SOCIAL_CIRCLE', 'OBS_60_CNT_SOCIAL_CIRCLE', 'DEF_60_CNT_SOCIAL_CIRCLE']

for column in OBS_DEF_columns:
    app_df[column].fillna(0, inplace=True)


#AMT_ANNUITY AMT_GOODS_PRICE
#These are continuous variables, we check for skewness

app_df['AMT_ANNUITY'].skew()


app_df['AMT_GOODS_PRICE'].skew()





app_df['AMT_ANNUITY'].fillna(app_df['AMT_ANNUITY'].median(),inplace=True)
app_df['AMT_GOODS_PRICE'].fillna(app_df['AMT_GOODS_PRICE'].median(),inplace=True)


#CNT_FAM_MEMBERS
#This is a numeric count column, we replace with Mode

app_df.CNT_FAM_MEMBERS.fillna(app_df.CNT_FAM_MEMBERS.mode()[0] , inplace = True)


#DAYS_LAST_PHONE_CHANGE

app_df['DAYS_LAST_PHONE_CHANGE'].head(5)


'''
This is a numeric column, 
1. values will be standardized by taking absolute values
2. replace few missing values with Mode
'''
app_df['DAYS_LAST_PHONE_CHANGE']=app_df['DAYS_LAST_PHONE_CHANGE'].abs()

app_df['DAYS_LAST_PHONE_CHANGE'].fillna(app_df['DAYS_LAST_PHONE_CHANGE'].mode,inplace=True)


final_null_check = (app_df.isnull().sum()/len(app_df)*100).sort_values(ascending = False)
final_null_check








#Due to observation on "DAYS_LAST_PHONE_CHANGE" column, there is a need to inspect similar columns with days

app_df_days = [i for i in app_df if i.startswith('DAYS')]

app_df_days.remove("DAYS_LAST_PHONE_CHANGE")


app_df[app_df_days]





app_df[app_df_days] = abs(app_df[app_df_days])

app_df[app_df_days].head()





#GENDER

app_df['CODE_GENDER'].value_counts()


#We replace the XNA values with 'F' which is the highest occuring gender

app_df.loc[app_df.CODE_GENDER == 'XNA','CODE_GENDER'] = 'F' 
app_df['CODE_GENDER'].value_counts()


#ORGANIZATION_TYPE

app_df['ORGANIZATION_TYPE'].value_counts().head()


app_df[['ORGANIZATION_TYPE','NAME_INCOME_TYPE']].head(20)





app_df['ORGANIZATION_TYPE'] = app_df['ORGANIZATION_TYPE'].replace('XNA', 'Pensioner')
app_df['ORGANIZATION_TYPE'].value_counts().head()


DATA ENGINEERING - BINNING AND GROUPING


#DAYS_BIRTH

app_df['DAYS_BIRTH']= (app_df['DAYS_BIRTH']/365).astype(int)
app_df['DAYS_BIRTH'].unique()


app_df['AGE_GROUP']=pd.cut(app_df['DAYS_BIRTH'],                 
                         bins=[19,25,40,60,100], labels=['Very_Young','Youth', 'Middle_Age', 'Elder'])


app_df[['DAYS_BIRTH','AGE_GROUP']]


#AMT_CREDIT

app_df['AMT_CREDIT'].describe()


#Bin into groups

bin_edges = [45000, 270000, 513531, 808650, 1200000, 1600000, 2000000, 2500000, 4050000]
bin_labels = ['45k-270k', '270k-513.5k', '513.5k-808.65k', '808.65k-1.2M', '1.2M-1.6M', '1.6M-2M', '2M-2.5M', '2.5M-4.05M']


# Create a new column 'AMT_CREDIT_GROUP' based on the bins and labels

app_df['AMT_CREDIT_GROUP'] = pd.cut(app_df['AMT_CREDIT'], bins=bin_edges, labels=bin_labels, right=False)


app_df['AMT_CREDIT_GROUP'].value_counts()


#AMT_INCOME_TOTAL

app_df.AMT_INCOME_TOTAL.describe()


income_bins = [25650, 75000, 112500, 135000, 157500, 180000, 202500, 225000, 117000000]
income_labels = ["26k to 75k", "75k to 112.5k", "112.5k to 135k", "135k to 157.5k", "157.5k to 180k", "180k to 202.5k", "202.5k to 225k", "225k to 117M"]


# Creating a new column INCOME_GROUP
app_df['AMT_INCOME_GROUP'] = pd.cut(app_df['AMT_INCOME_TOTAL'], bins=income_bins, labels=income_labels)


app_df.AMT_INCOME_GROUP.value_counts()





#using a heatmap to show correlation

fig, ax = plt.subplots(figsize=(12,12))
mask = np.triu(np.ones_like(app_df.corr(numeric_only=True), dtype=bool))
sns.heatmap(app_df.corr(numeric_only=True), linewidths=0.5, mask=mask, square=True, ax=ax, annot=True);


#CORRELATED COLUMNS WITH TARGET

correlation=app_df.corr(numeric_only=True)
correlation.TARGET.sort_values(ascending=True).head()






app_df.drop(columns=["EXT_SOURCE_2","EXT_SOURCE_3"],inplace=True,axis=1)
app_df.shape





app_df.info()


#The datatype of categorical columns below will be changed to category to suit univariate analysis
app_df['NAME_CONTRACT_TYPE'] = app_df['NAME_CONTRACT_TYPE'].astype('category')
app_df['CODE_GENDER'] = app_df['CODE_GENDER'].astype('category')
app_df['NAME_TYPE_SUITE'] = app_df['NAME_TYPE_SUITE'].astype('category')
app_df['NAME_INCOME_TYPE'] = app_df['NAME_INCOME_TYPE'].astype('category')
app_df['NAME_EDUCATION_TYPE'] = app_df['NAME_EDUCATION_TYPE'].astype('category')
app_df['NAME_FAMILY_STATUS'] = app_df['NAME_FAMILY_STATUS'].astype('category')
app_df['NAME_HOUSING_TYPE'] = app_df['NAME_HOUSING_TYPE'].astype('category')
app_df['OCCUPATION_TYPE'] = app_df['OCCUPATION_TYPE'].astype('category')
app_df['WEEKDAY_APPR_PROCESS_START'] = app_df['WEEKDAY_APPR_PROCESS_START'].astype('category')
app_df['ORGANIZATION_TYPE'] = app_df['ORGANIZATION_TYPE'].astype('category')


#After observations, these columns are to be dropped due to less relevance for analysis

drop_this_columns =['FLAG_MOBIL', 'FLAG_EMP_PHONE', 'FLAG_WORK_PHONE', 'FLAG_CONT_MOBILE',
       'FLAG_PHONE', 'FLAG_EMAIL','REGION_RATING_CLIENT','REGION_RATING_CLIENT_W_CITY','FLAG_EMAIL', 'REGION_RATING_CLIENT',
       'REGION_RATING_CLIENT_W_CITY', 'FLAG_DOCUMENT_2', 'FLAG_DOCUMENT_3','FLAG_DOCUMENT_4', 'FLAG_DOCUMENT_5', 'FLAG_DOCUMENT_6',
       'FLAG_DOCUMENT_7', 'FLAG_DOCUMENT_8', 'FLAG_DOCUMENT_9','FLAG_DOCUMENT_10', 'FLAG_DOCUMENT_11', 'FLAG_DOCUMENT_12',
       'FLAG_DOCUMENT_13', 'FLAG_DOCUMENT_14', 'FLAG_DOCUMENT_15','FLAG_DOCUMENT_16', 'FLAG_DOCUMENT_17', 'FLAG_DOCUMENT_18',
       'FLAG_DOCUMENT_19', 'FLAG_DOCUMENT_20', 'FLAG_DOCUMENT_21']


app_df[drop_this_columns].head()


app_df.drop(labels=drop_this_columns,axis=1,inplace=True)


#GENDER DISTRIBUTION FOR NON-DEFAULTERS AND DEFAULTERS
Target0 = app_df.loc[app_df["TARGET"] == 0] 
Target1 = app_df.loc[app_df["TARGET"] == 1]

plt.figure(figsize=(15,8))
plt.subplot(121)
sns.countplot(x='TARGET',hue='CODE_GENDER',data=Target0, palette = 'Set2')
plt.title("Gender Distribution for Non-Defaulters")
plt.subplot(122)
sns.countplot(x='TARGET',hue='CODE_GENDER',data=Target1, palette = 'Set2')
plt.title("Gender Distribution for Defaulters")





#AGE DISTRIBUTION FOR NON-DEFAULTERS AND DEFAULTERS

plt.figure(figsize=(15,7)) 
plt.subplot(121)
sns.countplot(x='TARGET',hue='AGE_GROUP',data=Target0,palette='Set2')
plt.title("Age Distribution for Non-Defaulters")
plt.subplot(122)
sns.countplot(x='TARGET',hue='AGE_GROUP',data=Target1,palette='Set2')
plt.title("Age Distribution for Defaulters")
plt.show()





#CREDIT AMOUNT DISTRIBUTION FOR NON-DEFAULTERS AND DEFAULTERS

plt.figure(figsize=(15,7)) 
plt.subplot(121)
sns.countplot(x='TARGET',hue='AMT_CREDIT_GROUP',data=Target0,palette='Set2')
plt.title("Credit Amount Group Distribution for Non-Defaulters")
plt.subplot(122)
sns.countplot(x='TARGET',hue='AMT_CREDIT_GROUP',data=Target1,palette='Set2')
plt.title("Credit Amount Group Distribution for Defaulters")
plt.show()








categorical_col = list(app_df.select_dtypes(include= 'category').columns) 

# Removing CODE_GENDER','AGE_GROUP', 'AMT_CREDIT_GROUP' because we have already taken up the insights from  above plots
categorical_col.remove('CODE_GENDER')
categorical_col.remove('AGE_GROUP')
categorical_col.remove('AMT_CREDIT_GROUP') 

categorical_col


def categorical_plot(var):
    plt.figure(figsize=(40,20))
    
    plt.rcParams['axes.labelpad'] = 50
    plt.subplot(1, 2, 1)
    sns.countplot(x='TARGET', hue=var, data=Target0, palette = 'Set3') 
    plt.xlabel(var, fontsize= 30, fontweight="bold")                                                    
    plt.ylabel('NON DEFAULTERS', fontsize= 30, fontweight="bold")
    plt.xticks(rotation=90, fontsize=30)
    plt.yticks(rotation=360, fontsize=30)
    
    
    plt.rcParams['axes.labelpad'] = 50
    plt.subplot(1, 2, 2)
    sns.countplot(x='TARGET', hue=var, data=Target1, palette = 'Set1') 
    plt.xlabel(var, fontsize= 30, fontweight="bold")
    plt.ylabel('DEFAULTERS', fontsize= 30, fontweight="bold")
    plt.xticks(rotation=90, fontsize=30)
    plt.yticks(rotation=360, fontsize=30)
    plt.show()


for col in categorical_col:
    categorical_plot(col)








def numerical_plot(col):
    sns.set(style="darkgrid")
    plt.figure(figsize=(40,20))
    
   
    plt.subplot(1,2,1)                                   
    sns.distplot(Target0[col], color="g" )
    plt.yscale('linear') 
    plt.xlabel(col, fontsize= 30, fontweight="bold")
    plt.ylabel('NON DEFAULTERS', fontsize= 30, fontweight="bold")                  
    plt.xticks(rotation=90, fontsize=30)
    plt.yticks(rotation=360, fontsize=30)
     
    
    
    
    plt.subplot(1,2,2)                                                                                                      
    sns.distplot(Target1[col], color="r")
    plt.yscale('linear')    
    plt.xlabel(col, fontsize= 30, fontweight="bold")
    plt.ylabel('DEFAULTERS', fontsize= 30, fontweight="bold")                     
    plt.xticks(rotation=90, fontsize=30)
    plt.yticks(rotation=360, fontsize=30)
    
    plt.show();


numerical_plot(col='AMT_CREDIT')


numerical_plot('AMT_INCOME_TOTAL')


numerical_plot('AMT_ANNUITY')








app_df[["TARGET","AMT_INCOME_TOTAL","NAME_EDUCATION_TYPE","NAME_FAMILY_STATUS"]]


#NAME_EDUCATION_TYPE VS AMT_INCOME_TOTAL VS NAME_FAMILY_STATUS

#As the values are too large, it is better to use log for better analysis
plt.figure(figsize=(35,14)) 
plt.yscale('log')                     
plt.xticks(rotation = 90)

#Boxplot w.r.t Data Target 0
sns.boxplot(data =Target0, x='NAME_EDUCATION_TYPE',y='AMT_INCOME_TOTAL',   
            hue ='NAME_FAMILY_STATUS',orient='v',palette='Set2')

#Adjusting legend position
plt.legend( loc = 'upper right')                                             
plt.title('Income amount vs Education Status',fontsize=35 )
plt.xlabel("NAME_EDUCATION_TYPE",fontsize= 30, fontweight="bold")
plt.ylabel("AMT_INCOME_TOTAL",fontsize= 30, fontweight="bold")
plt.xticks(rotation=90, fontsize=30)
plt.yticks(rotation=360, fontsize=30)

plt.show()



